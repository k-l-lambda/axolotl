{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['https_proxy'] = 'http://172.17.0.1:1081'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/axolotl/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Instantiating LlamaAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EaModel(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (ea_layer): Model(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "    (act): SiLU()\n",
       "    (logsoftmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from axolotl.models.eagle import EaModel\n",
    "\n",
    "\n",
    "model = EaModel.from_pretrained(base_model_path='/models/Meta-Llama-3-8B-Instruct', ea_model_path='yuhuili/EAGLE-LLaMA3-Instruct-8B')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embed_tokens): Embedding(128256, 4096, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "  (act): SiLU()\n",
       "  (logsoftmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ea_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 381]),\n",
       " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    264,  12514,\n",
       "           15592,  18328,  10968,    389,   8405,  13687,     11,  49150,     11,\n",
       "             323,  33445,  11503,     13,   4718,   9131,    374,    311,   1520,\n",
       "            3932,    304,    264,   6220,    323,  54584,   1648,     13,  24119,\n",
       "            6106,    701,  14847,    527,   1949,    505,  28856,     11,  89735,\n",
       "              11,  68763,     11,    477,  12079,   2262,     11,    323,  37106,\n",
       "             311,  10519,    264,   6928,    323,  74315,  13356,    627,   4599,\n",
       "           17011,    449,  25420,    477,    304,   1030,  38638,   4860,     11,\n",
       "            1935,    279,   6776,    311,  38263,    279,   4360,   4619,    315,\n",
       "           10209,  15465,   2038,     13,   1442,    499,    527,  44003,    922,\n",
       "             459,   4320,     11,  25670,    701,   9669,    323,   5766,  34000,\n",
       "           65383,    905,   2038,     13,   4718,   5915,    374,    311,  31087,\n",
       "             264,  11190,    323,  39319,  21976,     13, 128009, 128006,    882,\n",
       "          128007,    271, 108686,  61786,  32335, 110085, 123092, 111766,  21043,\n",
       "          112471, 128009, 128006,  78191, 128007,    271, 108686,  61786,  10110,\n",
       "              43,    290,    301,  71105,   7705, 107226,  25129, 102700,  13372,\n",
       "            9554, 117366, 114253,  43323, 102981,   9554, 111766, 116192, 108399,\n",
       "          119796, 117366,  98220,   9554,  53283,  13372,  17792, 101559,   1811,\n",
       "          108855,  37046,  81543,  35056, 102769,   9554,  28469, 102378,  88852,\n",
       "          107226,  98184,  43511,  58318, 117366,  98220,   9554, 111766,  49543,\n",
       "              16,     13,   3146, 101545,  70349, 101011, 112616, 101630, 110041,\n",
       "           14260, 104189, 106625, 100179,  43240,    334,  10110,     34,   2889,\n",
       "           13389,  65515, 123407, 108686,  61786,   9554,  33764,  46034,  34208,\n",
       "           42016,  23039, 104587,  21043, 126585,  81180,    226, 109895, 106444,\n",
       "           83266,   9554,  83266,  46961,   9174,     17,     13,   3146,  51385,\n",
       "           68464, 103420, 105665,  14260,  21601,  61786, 101630,    334,  10110,\n",
       "           17555,  82513,  85341, 123407, 103478, 102831, 120998, 106444,  83266,\n",
       "            9554,  83266,  98915,  34208, 111766,   9174,     18,     13,   3146,\n",
       "           51385, 118287, 100179,  14260, 108620, 102283,  70349, 101630,    334,\n",
       "           10110,  44801,    983,    301,   7923,  83305, 123407, 103478, 102831,\n",
       "          120998, 106444,  83266,   9554,  83266,  98915,  34208, 103693, 119372,\n",
       "          110952,   9080, 108870, 112948,  83266,   9554,  83266,  98915,   9174,\n",
       "              19,     13,   3146, 102283,  70626, 101011,  14260, 104189, 121478,\n",
       "             334,  10110,  48741,    437,  12093,   7453, 123407, 103478, 102831,\n",
       "          120998, 106444,  83266,   9554,  83266,  98915,  34208, 112948, 127833,\n",
       "          117540,  50182,  83266,   9554,  83266,  98915,   9174,     20,     13,\n",
       "            3146, 113536, 108481,  75146, 100179,  14260,  21601,  57106, 101630,\n",
       "          100179,    334,  10110,     36,   3059,    447,  13327,  12471,    352,\n",
       "          123407, 103478, 102831, 120998, 106444,  83266,   9554,  83266,  98915,\n",
       "           34208, 108481, 104083,  60632, 101630,  83266,   9554,  83266,  98915,\n",
       "            3490, 108787,  17792, 108619, 117366,  98220,   9554,  53283,  13372,\n",
       "          113679, 104587,  21043, 108686,  61786,   9554, 111766,  34208,  83266,\n",
       "           98915,   1811, 128009]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "prompt = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a dedicated AI assistant focused on providing accurate, respectful, and supportive answers. Your mission is to help users in a safe and constructive way. Always ensure your responses are free from harmful, unethical, discriminatory, or illegal content, and strive to maintain a positive and unbiased perspective.\n",
    "When faced with unclear or incoherent questions, take the opportunity to clarify the issue instead of offering incorrect information. If you are unsure about an answer, acknowledge your limitations and avoid disseminating false information. Your goal is to foster a helpful and informative dialogue.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "æ¢…è¥¿æœ€å¥½çš„å‡ ä¸ªæœ‹å‹æ˜¯è°<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "æ¢…è¥¿ï¼ˆLionel Messiï¼‰æ˜¯ä¸€ä½è‘—åçš„è¶³çƒè¿åŠ¨å‘˜ï¼Œä»–çš„æœ‹å‹åœˆåŒ…æ‹¬è®¸å¤šè¶³çƒç•Œçš„çŸ¥åäººå£«ã€‚è™½ç„¶æˆ‘æ²¡æœ‰ç¡®åˆ‡çš„ä¿¡æ¯ï¼Œä½†ä»¥ä¸‹æ˜¯ä¸€äº›ä»–ä¸è¶³çƒç•Œçš„æœ‹å‹ï¼š\n",
    "\n",
    "1. **å…‹é‡Œæ–¯è’‚äºšè¯ºÂ·ç½—çº³å°”å¤š**ï¼ˆCristiano Ronaldoï¼‰ï¼šæ¢…è¥¿çš„å¯¹æ‰‹å’ŒåŒè¡Œï¼Œä¹Ÿæ˜¯è‘¡è„ç‰™å›½å®¶é˜Ÿçš„é˜Ÿé•¿ã€‚\n",
    "2. **å®‰ä¸œå°¼å¥¥Â·åŠ è¥¿äºš**ï¼ˆAntonio GarcÃ­aï¼‰ï¼šé˜¿æ ¹å»·å›½å®¶é˜Ÿçš„é˜Ÿå‹å’Œæœ‹å‹ã€‚\n",
    "3. **å®‰èµ«å°”Â·è¿ªé©¬é‡Œäºš**ï¼ˆÃngel Di MarÃ­aï¼‰ï¼šé˜¿æ ¹å»·å›½å®¶é˜Ÿçš„é˜Ÿå‹å’Œå·´é»åœ£æ—¥è€³æ›¼é˜Ÿçš„é˜Ÿå‹ã€‚\n",
    "4. **é©¬ç§‘æ–¯Â·ç½—éœ**ï¼ˆMarcos Rojoï¼‰ï¼šé˜¿æ ¹å»·å›½å®¶é˜Ÿçš„é˜Ÿå‹å’Œæ›¼å½»æ–¯ç‰¹è”é˜Ÿçš„é˜Ÿå‹ã€‚\n",
    "5. **åŸƒå¡åŸºå°”Â·åŠ æ¯”äºšå°”**ï¼ˆEzequiel Garayï¼‰ï¼šé˜¿æ ¹å»·å›½å®¶é˜Ÿçš„é˜Ÿå‹å’Œå¡ç»´åˆ©äºšé˜Ÿçš„é˜Ÿå‹ã€‚\n",
    "\n",
    "è¿™äº›äººéƒ½æ˜¯è¶³çƒç•Œçš„çŸ¥åäººç‰©ï¼Œä¹Ÿæ˜¯æ¢…è¥¿çš„æœ‹å‹å’Œé˜Ÿå‹ã€‚<|eot_id|>'''\n",
    "\n",
    "tokens = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "tokens.shape, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 381, 4096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = model.base_model.model(tokens).last_hidden_state\n",
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 380, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = hidden_state[:, :-1] #.roll(shifts=1, dims=1)\n",
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 380, 4096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_hs = model.ea_layer(last_hidden_state, input_ids=tokens[:, 1:])\n",
    "out_hs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 380, 128256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.base_model.lm_head(out_hs)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   220,    258,    198,    220,     11,    264,  52067,     11,  18328,\n",
       "             11,     11,   6968,  35649,   2038,   1664,     11,    323,   9959,\n",
       "          10105,    311, 128009,  13291,    374,    311,   3493,   3932,  11322,\n",
       "            872,     11,     11,  49150,  11827,     11, 128009,    387,    430,\n",
       "          14847,    374,  49150,    323,  15837,    323,  15837,     11,    477,\n",
       "             11,    477,    904,  12659,    382,    323,  12192,    311,  10519,\n",
       "           6928,  49150,  16630,  49150,  16630,    382, 128009,    358,    264,\n",
       "            264,    477,  55861,  55861,  19684,   7540,     11,    358,   8475,\n",
       "           5995,    311,  38263,    323,   1217,    323,    315,   5042,  15465,\n",
       "          11503,    477, 128009,   1070,    527,  44003,    315,    279,   4320,\n",
       "            477,   6056,    701,  27924,    323,   3493,    279,  65383,   2038,\n",
       "           2038,    382, 128009,   5915,    374,    311,   3493,   7095,  49150,\n",
       "             11,  49150,   4676,    430, 128009, 128009,  18328,  78191,    323,\n",
       "             40,     11,    320,   3922,  10110,  79059,   9554,  11571,  11571,\n",
       "         128006,    271,  11571, 128007,   2675,  31809,  61786,  85523,    290,\n",
       "          71105,  71105,      8,  21043,  19483,  13372,  34208,   9554, 117366,\n",
       "           6447,  79059,   3922,  19361,  43240,  34208,  21043,  43240,   9554,\n",
       "          34208,  34208,  34208,  34208,  34208,  34208,   1811,  88852,  21043,\n",
       "          81543,  93233,   9554,   9554,  28469, 102378,  43511,  21043,  98184,\n",
       "          98184,  34208,  53283,   9554,  34208,  53283,  34208,     16,     13,\n",
       "         104829,     55,   5232,  34208,  34208,   9554,   5232,   5232, 104189,\n",
       "         104189,  10110,   5232,  10110,  10110, 100215,   2889,  13389, 100215,\n",
       "           7705,  21043,  34208,  34208,  34208,  34208,  34208,  42016,  34208,\n",
       "          23039,  21043, 104563,  34208,  34208,   9554,   9554,   9554,   9554,\n",
       "          34208,  34208,  46961,     17,     13,   3146,  12774,   5232,   5232,\n",
       "           5232,    334,  34208,   5232,   5232,  96618,  10110, 103478,  42825,\n",
       "          96618, 123407,  21043,  21043,   5232,  21043,   9554,  34208,  83266,\n",
       "          83266,  34208, 104563,   9174,     18,     13,   3146,  12774,  34208,\n",
       "          34208,    334,  10110,  34208,  34208,  70349,  96618,  10110,  51385,\n",
       "             75,    301,    409,  11644,    323,  10110,  34208,   9554,  34208,\n",
       "          34208,  34208,  83266,  83266,  34208,  25580,   9174,   9174,  70349,\n",
       "          34208,   9554,   9174,   9554,  83266,  83266,   9174,     19,     13,\n",
       "           3146,  34208,  35083,  11881,  34208,  34208,  34208,  34208,   5232,\n",
       "         102283,   3244, 123407,   2483,    323,  10110, 103478,   9554,   9554,\n",
       "          34208,   9554,  83266,  83266,   9174, 103478,   9174, 102769,   9174,\n",
       "           9174,  83266,  83266,  83266,   9174,     20,     13,   3146,  17039,\n",
       "         100179, 101011,    334,    334,  10110,  34208,  43511,   5232,    334,\n",
       "          10110, 103478,   1030,    447,  13327,    480, 114145,  42825,  10110,\n",
       "         103478, 120998,  34208, 106444,   9554,  83266,  98915,  34208,  25580,\n",
       "           3490,   3490, 101630,   9554,   9554,  83266,  83266,   3490, 108787,\n",
       "          21043,  80578,  16325,   9554,  34208,  53283,  34208,  34208,  34208,\n",
       "          21043,  34208,  61786,   9554, 111766,  34208,  42016,  83266,   3490,\n",
       "         128009, 128006]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ids = logits.argmax(dim=-1)\n",
    "out_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in\n",
      ", a chatting, assistant,, creating personalized information well, and relevant solutions to<|eot_id|> assistance is to provide users achieve their,, respectful manner,<|eot_id|> be that responses is respectful and bias and bias, or, or any practices.\n",
      "\n",
      " and promote to maintain positive respectful tone respectful tone.\n",
      "\n",
      "<|eot_id|> I a a or ambiguous ambiguousquate requests, I appropriate necessary to clarify and user and of simply incorrect answers or<|eot_id|> there are unsure of the answer or seek your uncertainty and provide theminating information information.\n",
      "\n",
      "<|eot_id|> goal is to provide trust respectful, respectful environment that<|eot_id|><|eot_id|> assistantassistant andI, (ï¼Œï¼ˆçƒçš„ï¼Ÿï¼Ÿ<|start_header_id|>\n",
      "\n",
      "ï¼Ÿ<|end_header_id|>Youå°è¥¿Mession Messi Messi)æ˜¯ä¸ªåå’Œçš„è¶³çƒï¼çƒï¼Œæœ‰å¤šå’Œæ˜¯å¤šçš„å’Œå’Œå’Œå’Œå’Œå’Œã€‚ä»¥ä¸‹æ˜¯æ²¡æœ‰æ‰¾çš„çš„ä¿¡æ¯ï¼Œä½†ä»–æ˜¯äº›äº›å’ŒçŸ¥çš„å’ŒçŸ¥å’Œ1.æXï¼šå’Œå’Œçš„ï¼šï¼šç½—ç½—ï¼ˆï¼šï¼ˆï¼ˆ Cristianoristiano Cristianoï¼‰æ˜¯å’Œå’Œå’Œå’Œå’ŒåŒå’Œè¡Œæ˜¯ä»–ä»¬å’Œå’Œçš„çš„çš„çš„å’Œå’Œé•¿2. **ï¿½ï¼šï¼šï¼š**å’Œï¼šï¼š**:ï¼ˆé˜¿Ã­o**:ï¼‰ï¼šæ˜¯æ˜¯ï¼šæ˜¯çš„å’Œé˜Ÿé˜Ÿå’Œä»–ä»¬ã€‚\n",
      "3. **ï¿½å’Œå’Œ**ï¼ˆå’Œå’Œé‡Œ**:ï¼ˆå®‰lel deÃ¡n andï¼ˆå’Œçš„å’Œå’Œå’Œé˜Ÿé˜Ÿå’Œå‰ã€‚\n",
      "ã€‚\n",
      "é‡Œå’Œçš„ã€‚\n",
      "çš„é˜Ÿé˜Ÿã€‚\n",
      "4. **å’Œæ ¼ï¿½å’Œå’Œå’Œå’Œï¼šé©¬Ã³nï¼‰ï¼šÃ­ andï¼ˆé˜¿çš„çš„å’Œçš„é˜Ÿé˜Ÿã€‚\n",
      "é˜¿ã€‚\n",
      "åˆ‡ã€‚\n",
      "ã€‚\n",
      "é˜Ÿé˜Ÿé˜Ÿã€‚\n",
      "5. **æ–°å°”æ–¯****ï¼ˆå’Œä»–ï¼š**ï¼ˆé˜¿coquiel GiddiÃ­oï¼ˆé˜¿å»·å’Œå›½å®¶çš„é˜Ÿå‹å’Œå‰ã€‚\n",
      "\n",
      "ã€‚\n",
      "\n",
      "äºšçš„çš„é˜Ÿé˜Ÿã€‚\n",
      "\n",
      "è¿™äº›æ˜¯ä»¬ä¸­çš„å’ŒçŸ¥å’Œå’Œå’Œæ˜¯å’Œè¥¿çš„æœ‹å‹å’ŒåŒé˜Ÿã€‚\n",
      "\n",
      "<|eot_id|><|start_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(101)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accept = out_ids[0, :-1] == tokens[0, 2:]\n",
    "accept.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
