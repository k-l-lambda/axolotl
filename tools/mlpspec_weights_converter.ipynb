{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm_head.weight': 'model-00004-of-00005.safetensors',\n",
       " 'mlp_model.speculator.emb.0.weight': 'model-00004-of-00005.safetensors',\n",
       " 'mlp_model.speculator.emb.1.weight': 'model-00004-of-00005.safetensors',\n",
       " 'mlp_model.speculator.emb.2.weight': 'model-00004-of-00005.safetensors',\n",
       " 'mlp_model.speculator.emb.3.weight': 'model-00004-of-00005.safetensors',\n",
       " 'mlp_model.speculator.head.0.weight': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.head.1.weight': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.head.2.weight': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.head.3.weight': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.ln.0.bias': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.ln.0.weight': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.ln.1.bias': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.ln.1.weight': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.ln.2.bias': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.ln.2.weight': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.ln.3.bias': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.ln.3.weight': 'model-00005-of-00005.safetensors',\n",
       " 'mlp_model.speculator.proj.0.weight': 'model-00004-of-00005.safetensors',\n",
       " 'mlp_model.speculator.proj.1.weight': 'model-00004-of-00005.safetensors',\n",
       " 'mlp_model.speculator.proj.2.weight': 'model-00004-of-00005.safetensors',\n",
       " 'mlp_model.speculator.proj.3.weight': 'model-00004-of-00005.safetensors',\n",
       " 'model.embed_tokens.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.0.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.1.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.10.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.10.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.10.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.10.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.10.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.10.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.10.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.10.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.10.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.11.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.12.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.13.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.14.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.15.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.16.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.17.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.18.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.19.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.2.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.2.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.2.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.2.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.2.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.2.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.2.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.2.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.2.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.20.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.20.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.20.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.20.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.20.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.20.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.20.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.20.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.20.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.21.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.21.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.21.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.21.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.21.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.21.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.21.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.21.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.21.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.22.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.23.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.24.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.25.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.26.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.27.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.28.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.29.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.3.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.3.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.3.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.3.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.3.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.3.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.3.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.3.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.3.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.30.input_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.30.mlp.down_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.30.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.30.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.30.post_attention_layernorm.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.30.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.30.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.30.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.30.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.31.input_layernorm.weight': 'model-00004-of-00005.safetensors',\n",
       " 'model.layers.31.mlp.down_proj.weight': 'model-00004-of-00005.safetensors',\n",
       " 'model.layers.31.mlp.gate_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.31.mlp.up_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.31.post_attention_layernorm.weight': 'model-00004-of-00005.safetensors',\n",
       " 'model.layers.31.self_attn.k_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.31.self_attn.o_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.31.self_attn.q_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.31.self_attn.v_proj.weight': 'model-00003-of-00005.safetensors',\n",
       " 'model.layers.4.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.4.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.4.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.4.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.4.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.4.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.4.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.4.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.4.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.5.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.6.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.7.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.input_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.mlp.down_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.mlp.gate_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.mlp.up_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.post_attention_layernorm.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.self_attn.k_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.self_attn.o_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.self_attn.q_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.8.self_attn.v_proj.weight': 'model-00001-of-00005.safetensors',\n",
       " 'model.layers.9.input_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.9.mlp.down_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.9.mlp.gate_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.9.mlp.up_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.9.post_attention_layernorm.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.9.self_attn.k_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.9.self_attn.o_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.9.self_attn.q_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.layers.9.self_attn.v_proj.weight': 'model-00002-of-00005.safetensors',\n",
       " 'model.norm.weight': 'model-00004-of-00005.safetensors'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "wmap = json.load(open('/models/train/20241103-mplspec-llama3.1-8b-instruct/model.safetensors.index.json', 'r'))['weight_map']\n",
    "wmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp_model.speculator.emb.0.weight',\n",
       " 'mlp_model.speculator.emb.1.weight',\n",
       " 'mlp_model.speculator.emb.2.weight',\n",
       " 'mlp_model.speculator.emb.3.weight',\n",
       " 'mlp_model.speculator.head.0.weight',\n",
       " 'mlp_model.speculator.head.1.weight',\n",
       " 'mlp_model.speculator.head.2.weight',\n",
       " 'mlp_model.speculator.head.3.weight',\n",
       " 'mlp_model.speculator.ln.0.bias',\n",
       " 'mlp_model.speculator.ln.0.weight',\n",
       " 'mlp_model.speculator.ln.1.bias',\n",
       " 'mlp_model.speculator.ln.1.weight',\n",
       " 'mlp_model.speculator.ln.2.bias',\n",
       " 'mlp_model.speculator.ln.2.weight',\n",
       " 'mlp_model.speculator.ln.3.bias',\n",
       " 'mlp_model.speculator.ln.3.weight',\n",
       " 'mlp_model.speculator.proj.0.weight',\n",
       " 'mlp_model.speculator.proj.1.weight',\n",
       " 'mlp_model.speculator.proj.2.weight',\n",
       " 'mlp_model.speculator.proj.3.weight']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_keys = [k for k in wmap.keys() if k.startswith('mlp_model.speculator.')]\n",
    "spec_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight', 'mlp_model.speculator.emb.0.weight', 'mlp_model.speculator.emb.1.weight', 'mlp_model.speculator.emb.2.weight', 'mlp_model.speculator.emb.3.weight', 'mlp_model.speculator.proj.0.weight', 'mlp_model.speculator.proj.1.weight', 'mlp_model.speculator.proj.2.weight', 'mlp_model.speculator.proj.3.weight'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "w4 = torch.load('/models/train/20241103-mplspec-llama3.1-8b-instruct/pytorch_model-00004-of-00005.bin')\n",
    "w4.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m w5 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/models/train/20241103-mplspec-llama3.1-8b-instruct/pytorch_model-00005-of-00005.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m w5\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/work/axolotl/env/lib/python3.10/site-packages/torch/serialization.py:1004\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1002\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1003\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m   1006\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1008\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/work/axolotl/env/lib/python3.10/site-packages/torch/serialization.py:456\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "w5 = torch.load('/models/train/20241103-mplspec-llama3.1-8b-instruct/pytorch_model-00005-of-00005.bin')\n",
    "w5.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mlp_model.speculator.head.0.weight', 'mlp_model.speculator.head.1.weight', 'mlp_model.speculator.head.2.weight', 'mlp_model.speculator.head.3.weight', 'mlp_model.speculator.ln.0.bias', 'mlp_model.speculator.ln.0.weight', 'mlp_model.speculator.ln.1.bias', 'mlp_model.speculator.ln.1.weight', 'mlp_model.speculator.ln.2.bias', 'mlp_model.speculator.ln.2.weight', 'mlp_model.speculator.ln.3.bias', 'mlp_model.speculator.ln.3.weight'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "\n",
    "w5 = dict()\n",
    "with safe_open(\"/models/train/20241103-mplspec-llama3.1-8b-instruct/checkpoint-79737/model-00005-of-00005.safetensors\", 'pt', device='cpu') as f:\n",
    "\tfor key in f.keys():\n",
    "\t\tw5[key] = f.get_tensor(key)\n",
    "\n",
    "w5.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('mlp_model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speculator.emb.0.weight': tensor([[-3.0640e-02,  5.2979e-02, -2.5586e-01,  ..., -2.5269e-02,\n",
       "           1.4465e-02,  1.8311e-02],\n",
       "         [-2.9541e-02,  2.3071e-02, -1.8066e-01,  ..., -1.5039e-01,\n",
       "          -6.5918e-02, -2.2559e-01],\n",
       "         [-1.2061e-01,  4.5654e-02, -2.6367e-01,  ..., -1.0059e-01,\n",
       "           8.8379e-02, -3.7354e-02],\n",
       "         ...,\n",
       "         [-1.3306e-02, -3.7354e-02, -8.7280e-03,  ...,  2.0386e-02,\n",
       "           2.7618e-03,  1.4587e-02],\n",
       "         [-1.6846e-02, -1.5545e-04, -2.8381e-03,  ..., -9.4604e-03,\n",
       "           4.6692e-03, -1.1963e-02],\n",
       "         [ 2.2095e-02,  3.0029e-02,  1.4893e-02,  ..., -1.0315e-02,\n",
       "           9.2163e-03,  2.4902e-02]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'speculator.emb.1.weight': tensor([[ 0.0040,  0.0061,  0.0752,  ...,  0.0591,  0.1768,  0.0664],\n",
       "         [-0.2285, -0.0471, -0.0427,  ..., -0.0226,  0.1914, -0.0025],\n",
       "         [-0.2617, -0.1465,  0.0522,  ...,  0.0018,  0.0408,  0.0938],\n",
       "         ...,\n",
       "         [-0.0160, -0.0237,  0.0025,  ..., -0.0140, -0.0199, -0.0120],\n",
       "         [-0.0142, -0.0017,  0.0129,  ..., -0.0019,  0.0302, -0.0154],\n",
       "         [-0.0121, -0.0068, -0.0073,  ..., -0.0183,  0.0154, -0.0205]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'speculator.emb.2.weight': tensor([[ 0.1689, -0.2637, -0.0187,  ..., -0.0801,  0.0203, -0.0972],\n",
       "         [-0.0649, -0.2598,  0.1230,  ...,  0.0693, -0.0718,  0.1729],\n",
       "         [-0.0031, -0.1484,  0.0542,  ..., -0.0094, -0.0067, -0.0483],\n",
       "         ...,\n",
       "         [-0.0054, -0.0034,  0.0140,  ..., -0.0349,  0.0039,  0.0121],\n",
       "         [-0.0327, -0.0320, -0.0258,  ...,  0.0273, -0.0125, -0.0084],\n",
       "         [-0.0041,  0.0179,  0.0282,  ..., -0.0028,  0.0148,  0.0131]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'speculator.emb.3.weight': tensor([[ 0.1318, -0.0413, -0.0388,  ..., -0.0496, -0.1250, -0.1279],\n",
       "         [-0.0206,  0.1416,  0.1826,  ..., -0.0413, -0.0684,  0.1270],\n",
       "         [ 0.1021, -0.0024,  0.0298,  ...,  0.0267,  0.0522, -0.2773],\n",
       "         ...,\n",
       "         [-0.0027, -0.0259,  0.0140,  ..., -0.0204, -0.0050, -0.0015],\n",
       "         [-0.0131, -0.0097,  0.0078,  ..., -0.0366, -0.0186,  0.0310],\n",
       "         [ 0.0128,  0.0090, -0.0077,  ...,  0.0125,  0.0293, -0.0457]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'speculator.head.0.weight': tensor([[-0.0095, -0.1475, -0.1699,  ..., -0.0393,  0.0312, -0.0986],\n",
       "         [ 0.0400,  0.1338, -0.0361,  ...,  0.1426, -0.0309, -0.0835],\n",
       "         [-0.1328, -0.0342, -0.0081,  ..., -0.1562, -0.2080, -0.1455],\n",
       "         ...,\n",
       "         [-0.0625, -0.0684, -0.0664,  ..., -0.0762, -0.0742, -0.1006],\n",
       "         [-0.0625, -0.0583, -0.0698,  ..., -0.0781, -0.0713, -0.0947],\n",
       "         [-0.0540, -0.0693, -0.0674,  ..., -0.0747, -0.0723, -0.1006]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'speculator.head.1.weight': tensor([[ 0.2500,  0.0530,  0.1670,  ..., -0.0503, -0.0078,  0.1641],\n",
       "         [-0.1250,  0.2178, -0.0408,  ..., -0.0493,  0.0605,  0.1836],\n",
       "         [-0.2559, -0.1846, -0.0396,  ..., -0.1924,  0.2383, -0.1338],\n",
       "         ...,\n",
       "         [-0.0625, -0.0596, -0.0625,  ..., -0.0625, -0.1104, -0.0510],\n",
       "         [-0.0562, -0.0588, -0.0625,  ..., -0.0625, -0.1187, -0.0610],\n",
       "         [-0.0544, -0.0347, -0.0625,  ..., -0.0645, -0.1099, -0.0625]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'speculator.head.2.weight': tensor([[ 0.0070,  0.0415,  0.2354,  ...,  0.0674,  0.0217, -0.0728],\n",
       "         [ 0.1816, -0.2480,  0.0737,  ..., -0.0859, -0.0415, -0.0996],\n",
       "         [ 0.2031, -0.2500, -0.0757,  ..., -0.0525, -0.2500, -0.1270],\n",
       "         ...,\n",
       "         [-0.0605,  0.0625, -0.0330,  ..., -0.0625, -0.0481, -0.0635],\n",
       "         [-0.0410,  0.0625, -0.0361,  ..., -0.0640, -0.0625, -0.0625],\n",
       "         [-0.0408,  0.0649, -0.0571,  ..., -0.0625, -0.0398, -0.0588]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'speculator.head.3.weight': tensor([[-0.1367, -0.0126,  0.0723,  ..., -0.0070,  0.2236, -0.0217],\n",
       "         [-0.0864,  0.0469, -0.1826,  ...,  0.1719, -0.0588, -0.0591],\n",
       "         [-0.1191, -0.0332, -0.1157,  ...,  0.2422,  0.0330, -0.2617],\n",
       "         ...,\n",
       "         [-0.0674, -0.0669, -0.0488,  ..., -0.0625, -0.0674, -0.0669],\n",
       "         [-0.0674, -0.0674, -0.0347,  ..., -0.0645, -0.0645, -0.0698],\n",
       "         [-0.0654, -0.0635, -0.0527,  ..., -0.0576, -0.0645, -0.0659]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'speculator.ln.0.bias': tensor([-0.1445, -0.1270, -0.1206,  ..., -0.1226, -0.1191, -0.1309],\n",
       "        dtype=torch.bfloat16),\n",
       " 'speculator.ln.0.weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.bfloat16),\n",
       " 'speculator.ln.1.bias': tensor([-0.0869, -0.1187, -0.1187,  ..., -0.0933, -0.0806, -0.0903],\n",
       "        dtype=torch.bfloat16),\n",
       " 'speculator.ln.1.weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.bfloat16),\n",
       " 'speculator.ln.2.bias': tensor([-0.1064, -0.1396, -0.0530,  ..., -0.0601, -0.0142, -0.0654],\n",
       "        dtype=torch.bfloat16),\n",
       " 'speculator.ln.2.weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.bfloat16),\n",
       " 'speculator.ln.3.bias': tensor([-0.0776, -0.0413, -0.0483,  ..., -0.0654,  0.0106, -0.1143],\n",
       "        dtype=torch.bfloat16),\n",
       " 'speculator.ln.3.weight': tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.bfloat16),\n",
       " 'speculator.proj.0.weight': tensor([[ 0.1118,  0.1309,  0.0767,  ..., -0.1250, -0.0354, -0.1689],\n",
       "         [-0.0122, -0.0840, -0.0894,  ...,  0.0513,  0.0483, -0.0986],\n",
       "         [-0.0055, -0.1357,  0.0361,  ..., -0.0055,  0.1396, -0.0574],\n",
       "         ...,\n",
       "         [-0.0044,  0.1006,  0.1079,  ..., -0.1157,  0.0225,  0.0259],\n",
       "         [ 0.0625,  0.1250,  0.0371,  ..., -0.0835,  0.1001,  0.0156],\n",
       "         [ 0.0261, -0.0771,  0.0284,  ..., -0.0315, -0.0796,  0.0366]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'speculator.proj.1.weight': tensor([[-0.1206, -0.1641,  0.1030,  ..., -0.0933, -0.1787, -0.0918],\n",
       "         [-0.0039,  0.0444,  0.0330,  ..., -0.0591, -0.0276,  0.0747],\n",
       "         [ 0.1719,  0.1406, -0.0884,  ...,  0.1279,  0.0894,  0.0264],\n",
       "         ...,\n",
       "         [ 0.0063, -0.0679, -0.1680,  ...,  0.0923, -0.1260, -0.0212],\n",
       "         [ 0.0024,  0.1631, -0.1211,  ..., -0.0435, -0.0737, -0.0449],\n",
       "         [-0.2275, -0.0615, -0.0505,  ...,  0.0659,  0.0281, -0.1260]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'speculator.proj.2.weight': tensor([[ 0.0146, -0.0396,  0.1147,  ...,  0.0972,  0.0325,  0.0361],\n",
       "         [-0.0732,  0.0894, -0.1172,  ...,  0.1299, -0.1592, -0.1631],\n",
       "         [-0.0024,  0.1719,  0.0630,  ..., -0.1230,  0.0162,  0.0115],\n",
       "         ...,\n",
       "         [ 0.0420,  0.1250, -0.2500,  ..., -0.0586, -0.0864, -0.0601],\n",
       "         [ 0.0596,  0.0339,  0.0918,  ...,  0.0608,  0.1250, -0.0923],\n",
       "         [ 0.0505, -0.1172, -0.0162,  ..., -0.0742, -0.0820,  0.0913]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'speculator.proj.3.weight': tensor([[ 0.0542,  0.1084,  0.1387,  ...,  0.0947, -0.0095,  0.0767],\n",
       "         [ 0.0243,  0.0403, -0.1069,  ...,  0.0508, -0.0098, -0.1250],\n",
       "         [ 0.1196,  0.0002, -0.0742,  ...,  0.0366, -0.1162, -0.1211],\n",
       "         ...,\n",
       "         [ 0.0957,  0.0061,  0.0654,  ..., -0.0220, -0.1011, -0.1201],\n",
       "         [ 0.0928, -0.0126,  0.0649,  ...,  0.1196,  0.0410, -0.0942],\n",
       "         [-0.0520,  0.0574,  0.1270,  ...,  0.1357,  0.0608,  0.1250]],\n",
       "        device='cuda:0', dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_map = {}\n",
    "\n",
    "for k in spec_keys:\n",
    "\tww = w4 if k in w4 else w5\n",
    "\tnew_map[k[10:]] = ww[k]\n",
    "\n",
    "new_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_map, '/models/train/20241103-mplspec-llama3.1-8b-instruct/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
