{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.weight torch.Size([128256, 8192])\n",
      "model.embed_tokens.weight torch.Size([128256, 8192])\n",
      "model.fc.weight torch.Size([8192, 16384])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([8192, 28672])\n",
      "model.layers.0.mlp.gate_up_proj.weight torch.Size([57344, 8192])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([8192])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([8192, 8192])\n",
      "model.layers.0.self_attn.qkv_proj.weight torch.Size([10240, 8192])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "\n",
    "with safe_open(\"/models/train/vllm-weights/v2/li-EAGLE-LLaMA3-Instruct-70B/model.safetensors\", 'pt', device='cpu') as f:\n",
    "\tfor key in f.keys():\n",
    "\t\tprint(key, f.get_tensor(key).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.weight torch.Size([128256, 4096])\n",
      "model.embed_tokens.weight torch.Size([128256, 4096])\n",
      "model.fc.bias torch.Size([4096])\n",
      "model.fc.weight torch.Size([4096, 8192])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.0.mlp.gate_up_proj.weight torch.Size([28672, 4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.qkv_proj.weight torch.Size([6144, 4096])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "\n",
    "with safe_open(\"/models/train/vllm-weights/v1/20240910-eagle-llama3.1-8b/new.safetensors\", 'pt', device='cpu') as f:\n",
    "\tfor key in f.keys():\n",
    "\t\tprint(key, f.get_tensor(key).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "indices = torch.load('/root/work/chat-regen/tools/frequent-tokens.pt')\n",
    "indices.shape, indices\n",
    "\n",
    "tail_indices = torch.arange(256).long() + 128000\n",
    "new_indices = torch.cat([indices[:32000], tail_indices], dim=0)\n",
    "new_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    11,    323,    279,  ..., 128253, 128254, 128255])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lm_head.weight': tensor([[ 0.0146,  0.0142, -0.0009,  ..., -0.0039, -0.0240, -0.0173],\n",
       "         [-0.0018,  0.0020,  0.0089,  ...,  0.0028, -0.0125, -0.0164],\n",
       "         [-0.0093,  0.0119, -0.0164,  ...,  0.0247, -0.0054, -0.0065],\n",
       "         ...,\n",
       "         [-0.0038,  0.0020,  0.0034,  ...,  0.0003,  0.0081,  0.0083],\n",
       "         [-0.0038,  0.0020,  0.0034,  ...,  0.0003,  0.0081,  0.0083],\n",
       "         [-0.0038,  0.0020,  0.0034,  ...,  0.0003,  0.0081,  0.0083]]),\n",
       " 'model.embed_tokens.weight': tensor([[-2.5558e-04, -4.1199e-04,  3.6478e-05,  ...,  2.1267e-04,\n",
       "           3.1471e-05,  2.9182e-04],\n",
       "         [-6.4087e-04, -2.9297e-03,  1.1673e-03,  ...,  6.7234e-05,\n",
       "           1.3962e-03,  5.1880e-03],\n",
       "         [ 5.0354e-04, -1.9989e-03, -1.3504e-03,  ...,  1.4572e-03,\n",
       "          -5.4321e-03,  3.0518e-03],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00],\n",
       "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          -0.0000e+00, -0.0000e+00]], dtype=torch.float16),\n",
       " 'model.fc.bias': tensor([-0.0894, -0.0373,  0.0781,  ..., -0.0701, -0.0330, -0.0367],\n",
       "        dtype=torch.float16),\n",
       " 'model.fc.weight': tensor([[ 3.5107e-01,  2.5342e-01, -1.4417e-01,  ...,  6.6638e-05,\n",
       "           9.6178e-04, -5.5275e-03],\n",
       "         [-5.4980e-01,  1.3623e-01,  8.1421e-02,  ...,  6.6223e-03,\n",
       "           2.0199e-03, -8.3694e-03],\n",
       "         [ 3.7354e-02,  9.8450e-02, -1.2396e-01,  ..., -8.3847e-03,\n",
       "           3.4561e-03,  2.2919e-02],\n",
       "         ...,\n",
       "         [ 2.7832e-01,  1.6309e-01, -2.9114e-02,  ...,  1.1987e-01,\n",
       "           1.6594e-03,  5.0888e-03],\n",
       "         [ 9.4116e-02,  4.7546e-02,  5.3680e-02,  ..., -7.5951e-03,\n",
       "           1.3342e-01,  2.1267e-03],\n",
       "         [ 1.4417e-01, -1.0101e-01, -2.4377e-01,  ...,  6.6299e-03,\n",
       "           7.4196e-03,  1.3171e-01]], dtype=torch.float16),\n",
       " 'model.layers.0.mlp.down_proj.weight': tensor([[-0.0350, -0.0229,  0.0128,  ..., -0.0032,  0.0336,  0.0173],\n",
       "         [-0.0366, -0.0143,  0.0510,  ...,  0.0201, -0.0075,  0.0100],\n",
       "         [-0.0359,  0.0252, -0.0320,  ..., -0.0311, -0.0341,  0.0230],\n",
       "         ...,\n",
       "         [ 0.0032,  0.0027, -0.0149,  ...,  0.0181, -0.0154,  0.0153],\n",
       "         [ 0.0219,  0.0278, -0.0031,  ..., -0.0035, -0.0500, -0.0439],\n",
       "         [-0.0015, -0.0037, -0.0298,  ..., -0.0203, -0.0112,  0.0197]],\n",
       "        dtype=torch.float16),\n",
       " 'model.layers.0.mlp.gate_up_proj.weight': tensor([[ 0.0047,  0.0025,  0.0269,  ...,  0.0084,  0.0307, -0.0034],\n",
       "         [-0.0157, -0.0014,  0.0253,  ..., -0.0106, -0.0325,  0.0026],\n",
       "         [-0.0393, -0.0151,  0.0227,  ...,  0.0213, -0.0069, -0.0119],\n",
       "         ...,\n",
       "         [-0.0036,  0.0065, -0.0075,  ..., -0.0054, -0.0132, -0.0398],\n",
       "         [-0.0038, -0.0206,  0.0023,  ...,  0.0457, -0.0122,  0.0182],\n",
       "         [ 0.0335,  0.0200, -0.0019,  ..., -0.0517,  0.0066,  0.0126]],\n",
       "        dtype=torch.float16),\n",
       " 'model.layers.0.post_attention_layernorm.weight': tensor([0.7246, 0.7178, 0.7065,  ..., 0.7251, 0.7695, 0.7236],\n",
       "        dtype=torch.float16),\n",
       " 'model.layers.0.self_attn.o_proj.weight': tensor([[-0.0070,  0.0204, -0.0061,  ..., -0.0013, -0.0294,  0.0255],\n",
       "         [ 0.0213,  0.0201, -0.0147,  ..., -0.0029,  0.0137, -0.0016],\n",
       "         [-0.0016, -0.0148, -0.0054,  ...,  0.0146,  0.0044,  0.0013],\n",
       "         ...,\n",
       "         [ 0.0225, -0.0005, -0.0034,  ...,  0.0011, -0.0089,  0.0067],\n",
       "         [ 0.0212,  0.0134,  0.0227,  ..., -0.0008,  0.0013, -0.0041],\n",
       "         [-0.0096, -0.0089, -0.0258,  ...,  0.0198, -0.0019,  0.0130]],\n",
       "        dtype=torch.float16),\n",
       " 'model.layers.0.self_attn.qkv_proj.weight': tensor([[ 0.0095, -0.0301,  0.0128,  ..., -0.0151,  0.0068, -0.0040],\n",
       "         [-0.0206,  0.0012,  0.0178,  ..., -0.0327,  0.0091,  0.0322],\n",
       "         [-0.0158,  0.0554, -0.0439,  ...,  0.0447, -0.0551,  0.0209],\n",
       "         ...,\n",
       "         [-0.0265,  0.0099,  0.0019,  ..., -0.0109, -0.0123, -0.0102],\n",
       "         [-0.0002,  0.0100, -0.0111,  ...,  0.0121,  0.0008, -0.0277],\n",
       "         [ 0.0042, -0.0183,  0.0244,  ...,  0.0050,  0.0066,  0.0142]],\n",
       "        dtype=torch.float16)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = dict()\n",
    "\n",
    "with safe_open(\"/models/train/vllm-weights/v1/20240910-eagle-llama3.1-8b/new.safetensors\", 'pt', device='cpu') as f:\n",
    "\tfor key in f.keys():\n",
    "\t\tw = f.get_tensor(key)\n",
    "\t\tif key in ['lm_head.weight', 'model.embed_tokens.weight']:\n",
    "\t\t\tstate[key] = w[new_indices]\n",
    "\t\telse:\n",
    "\t\t\tstate[key] = w\n",
    "\n",
    "\tprint(f.metadata())\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "\n",
    "save_file(state, \"/models/train/vllm-weights/v2/20240910-eagle-llama3.1-8b-vocab32k/model.safetensors\", metadata={'format': 'pt'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
